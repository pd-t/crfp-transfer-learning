{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2c86e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ray/anaconda3/lib/python3.8/site-packages (4.21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: requests in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ray/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ray/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f5885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (4.63.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (1.23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: aiohttp in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: packaging in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ray/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ray/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ray/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ray/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ray/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.13.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed datasets-2.9.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da895f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 24.7kB/s]\n",
      "Downloading config.json: 100%|██████████| 483/483 [00:00<00:00, 446kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 536kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 455k/455k [00:00<00:00, 863kB/s] \n",
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 267kB/s] \n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 266kB/s] \n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 259kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc to /home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data: 6.22kB [00:00, 3.11MB/s]\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:01<00:03,  1.57s/it]\n",
      "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
      "Downloading data: 19.6kB [00:00, 138kB/s]\u001b[A\n",
      "Downloading data: 54.4kB [00:00, 195kB/s]\u001b[A\n",
      "Downloading data: 124kB [00:00, 325kB/s] \u001b[A\n",
      "Downloading data: 264kB [00:00, 575kB/s]\u001b[A\n",
      "Downloading data: 1.05MB [00:00, 1.41MB/s][A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:03<00:01,  1.95s/it]\n",
      "Downloading data: 441kB [00:00, 29.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.59s/it]\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 947.08it/s]\n",
      "/tmp/ipykernel_44/2149597044.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'mrpc')\n",
      "Downloading builder script: 5.76kB [00:00, 3.03MB/s]                   \n",
      "100%|██████████| 4/4 [00:00<00:00, 27.89ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.26ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 32.07ba/s]\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/jovyan/.cache/huggingface/transformers/tmp5oaj_yav\n",
      "Downloading pytorch_model.bin: 100%|██████████| 256M/256M [00:04<00:00, 57.9MB/s] \n",
      "storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/jovyan/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "creating metadata file for /home/jovyan/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No `resources_per_trial` arg was passed into `hyperparameter_search`. Setting it to a default value of 1 CPU and 1 GPU for each trial.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 23:13:11,327\tWARNING utils.py:594 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2023-02-20 23:13:11,578\tWARNING services.py:1882 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 23:13:11,790\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py:642: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/ray-air/key-concepts.html#session\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:13 (running for 00:00:00.22)\n",
      "Memory usage on this node: 8.7/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "| Trial name             | status   | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |\n",
      "|------------------------+----------+------------------+-----------------+--------------------+------------------------+----------|\n",
      "| _objective_34426_00000 | RUNNING  | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |\n",
      "| _objective_34426_00001 | PENDING  |                  |     1.56207e-05 |                  2 |                     16 |  7.08379 |\n",
      "| _objective_34426_00002 | PENDING  |                  |     8.28892e-06 |                  5 |                     16 | 24.4435  |\n",
      "| _objective_34426_00003 | PENDING  |                  |     1.09943e-06 |                  2 |                      8 | 29.158   |\n",
      "| _objective_34426_00004 | PENDING  |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |\n",
      "| _objective_34426_00005 | PENDING  |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |\n",
      "| _objective_34426_00006 | PENDING  |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |\n",
      "| _objective_34426_00007 | PENDING  |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |\n",
      "| _objective_34426_00008 | PENDING  |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |\n",
      "| _objective_34426_00009 | PENDING  |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:20 (running for 00:00:07.37)\n",
      "Memory usage on this node: 14.0/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (7 PENDING, 3 RUNNING)\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "| Trial name             | status   | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |\n",
      "|------------------------+----------+------------------+-----------------+--------------------+------------------------+----------|\n",
      "| _objective_34426_00000 | RUNNING  | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |\n",
      "| _objective_34426_00001 | RUNNING  | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |\n",
      "| _objective_34426_00002 | RUNNING  | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |\n",
      "| _objective_34426_00003 | PENDING  |                  |     1.09943e-06 |                  2 |                      8 | 29.158   |\n",
      "| _objective_34426_00004 | PENDING  |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |\n",
      "| _objective_34426_00005 | PENDING  |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |\n",
      "| _objective_34426_00006 | PENDING  |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |\n",
      "| _objective_34426_00007 | PENDING  |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |\n",
      "| _objective_34426_00008 | PENDING  |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |\n",
      "| _objective_34426_00009 | PENDING  |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:25 (running for 00:00:12.38)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (7 PENDING, 3 RUNNING)\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "| Trial name             | status   | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |\n",
      "|------------------------+----------+------------------+-----------------+--------------------+------------------------+----------|\n",
      "| _objective_34426_00000 | RUNNING  | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |\n",
      "| _objective_34426_00001 | RUNNING  | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |\n",
      "| _objective_34426_00002 | RUNNING  | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |\n",
      "| _objective_34426_00003 | PENDING  |                  |     1.09943e-06 |                  2 |                      8 | 29.158   |\n",
      "| _objective_34426_00004 | PENDING  |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |\n",
      "| _objective_34426_00005 | PENDING  |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |\n",
      "| _objective_34426_00006 | PENDING  |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |\n",
      "| _objective_34426_00007 | PENDING  |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |\n",
      "| _objective_34426_00008 | PENDING  |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |\n",
      "| _objective_34426_00009 | PENDING  |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:30 (running for 00:00:17.38)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (7 PENDING, 3 RUNNING)\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "| Trial name             | status   | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |\n",
      "|------------------------+----------+------------------+-----------------+--------------------+------------------------+----------|\n",
      "| _objective_34426_00000 | RUNNING  | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |\n",
      "| _objective_34426_00001 | RUNNING  | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |\n",
      "| _objective_34426_00002 | RUNNING  | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |\n",
      "| _objective_34426_00003 | PENDING  |                  |     1.09943e-06 |                  2 |                      8 | 29.158   |\n",
      "| _objective_34426_00004 | PENDING  |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |\n",
      "| _objective_34426_00005 | PENDING  |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |\n",
      "| _objective_34426_00006 | PENDING  |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |\n",
      "| _objective_34426_00007 | PENDING  |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |\n",
      "| _objective_34426_00008 | PENDING  |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |\n",
      "| _objective_34426_00009 | PENDING  |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:35 (running for 00:00:22.39)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (7 PENDING, 3 RUNNING)\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "| Trial name             | status   | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |\n",
      "|------------------------+----------+------------------+-----------------+--------------------+------------------------+----------|\n",
      "| _objective_34426_00000 | RUNNING  | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |\n",
      "| _objective_34426_00001 | RUNNING  | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |\n",
      "| _objective_34426_00002 | RUNNING  | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |\n",
      "| _objective_34426_00003 | PENDING  |                  |     1.09943e-06 |                  2 |                      8 | 29.158   |\n",
      "| _objective_34426_00004 | PENDING  |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |\n",
      "| _objective_34426_00005 | PENDING  |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |\n",
      "| _objective_34426_00006 | PENDING  |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |\n",
      "| _objective_34426_00007 | PENDING  |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |\n",
      "| _objective_34426_00008 | PENDING  |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |\n",
      "| _objective_34426_00009 | PENDING  |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |\n",
      "+------------------------+----------+------------------+-----------------+--------------------+------------------------+----------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m {'train_runtime': 15.0171, 'train_samples_per_second': 488.508, 'train_steps_per_second': 20.377, 'train_loss': 0.5035776375165952, 'epoch': 2.0}\n",
      "Result for _objective_34426_00001:\n",
      "  date: 2023-02-20_23-13-36\n",
      "  done: true\n",
      "  epoch: 2.0\n",
      "  eval_accuracy: 0.8186274509803921\n",
      "  eval_f1: 0.875\n",
      "  eval_loss: 0.42614543437957764\n",
      "  eval_runtime: 0.3097\n",
      "  eval_samples_per_second: 1317.447\n",
      "  eval_steps_per_second: 164.681\n",
      "  experiment_id: 4ded468002664a058bc0a944202e54b2\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.6936274509803921\n",
      "  pid: 609\n",
      "  time_since_restore: 18.850216150283813\n",
      "  time_this_iter_s: 18.850216150283813\n",
      "  time_total_s: 18.850216150283813\n",
      "  timestamp: 1676963616\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00001'\n",
      "  warmup_time: 0.0031201839447021484\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=609)\u001b[0m {'eval_loss': 0.42614543437957764, 'eval_accuracy': 0.8186274509803921, 'eval_f1': 0.875, 'eval_runtime': 0.3097, 'eval_samples_per_second': 1317.447, 'eval_steps_per_second': 164.681, 'epoch': 2.0}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:42 (running for 00:00:28.82)\n",
      "Memory usage on this node: 14.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (6 PENDING, 3 RUNNING, 1 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00000 | RUNNING    | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |             |\n",
      "| _objective_34426_00002 | RUNNING    | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |             |\n",
      "| _objective_34426_00003 | RUNNING    | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |             |\n",
      "| _objective_34426_00004 | PENDING    |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | PENDING    |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | PENDING    |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m {'loss': 0.5325, 'learning_rate': 1.9438586275341763e-06, 'epoch': 3.27}\n",
      "Result for _objective_34426_00000:\n",
      "  date: 2023-02-20_23-13-44\n",
      "  done: false\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.7892156862745098\n",
      "  eval_f1: 0.8590163934426229\n",
      "  eval_loss: 0.4581927955150604\n",
      "  eval_runtime: 0.3175\n",
      "  eval_samples_per_second: 1285.232\n",
      "  eval_steps_per_second: 160.654\n",
      "  experiment_id: 816a1da0bc0b4a51aa0e1d0718bc1702\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.6482320797171326\n",
      "  pid: 579\n",
      "  time_since_restore: 28.820181846618652\n",
      "  time_this_iter_s: 28.820181846618652\n",
      "  time_total_s: 28.820181846618652\n",
      "  timestamp: 1676963624\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00000'\n",
      "  warmup_time: 0.0029206275939941406\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m {'eval_loss': 0.4581927955150604, 'eval_accuracy': 0.7892156862745098, 'eval_f1': 0.8590163934426229, 'eval_runtime': 0.3175, 'eval_samples_per_second': 1285.232, 'eval_steps_per_second': 160.654, 'epoch': 3.27}\n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m {'loss': 0.4874, 'learning_rate': 2.8713241434308047e-06, 'epoch': 3.27}\n",
      "Result for _objective_34426_00002:\n",
      "  date: 2023-02-20_23-13-46\n",
      "  done: false\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.8284313725490197\n",
      "  eval_f1: 0.8817567567567567\n",
      "  eval_loss: 0.43873196840286255\n",
      "  eval_runtime: 0.3098\n",
      "  eval_samples_per_second: 1316.919\n",
      "  eval_steps_per_second: 164.615\n",
      "  experiment_id: 050b42c640224f4d94fbfdb514687454\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.7101881293057764\n",
      "  pid: 611\n",
      "  time_since_restore: 28.391945600509644\n",
      "  time_this_iter_s: 28.391945600509644\n",
      "  time_total_s: 28.391945600509644\n",
      "  timestamp: 1676963626\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00002'\n",
      "  warmup_time: 0.003077268600463867\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m {'eval_loss': 0.43873196840286255, 'eval_accuracy': 0.8284313725490197, 'eval_f1': 0.8817567567567567, 'eval_runtime': 0.3098, 'eval_samples_per_second': 1316.919, 'eval_steps_per_second': 164.615, 'epoch': 3.27}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:51 (running for 00:00:37.95)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (6 PENDING, 3 RUNNING, 1 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00000 | RUNNING    | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00002 | RUNNING    | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | RUNNING    | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |             |\n",
      "| _objective_34426_00004 | PENDING    |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | PENDING    |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | PENDING    |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:13:56 (running for 00:00:42.96)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (6 PENDING, 3 RUNNING, 1 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00000 | RUNNING    | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00002 | RUNNING    | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | RUNNING    | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |             |\n",
      "| _objective_34426_00004 | PENDING    |                  |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | PENDING    |                  |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | PENDING    |                  |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(_objective pid=579)\u001b[0m {'train_runtime': 38.9352, 'train_samples_per_second': 471.039, 'train_steps_per_second': 19.648, 'train_loss': 0.49157021715750104, 'epoch': 5.0}\n",
      "Result for _objective_34426_00000:\n",
      "  date: 2023-02-20_23-13-44\n",
      "  done: true\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.7892156862745098\n",
      "  eval_f1: 0.8590163934426229\n",
      "  eval_loss: 0.4581927955150604\n",
      "  eval_runtime: 0.3175\n",
      "  eval_samples_per_second: 1285.232\n",
      "  eval_steps_per_second: 160.654\n",
      "  experiment_id: 816a1da0bc0b4a51aa0e1d0718bc1702\n",
      "  experiment_tag: 0_learning_rate=0.0000,num_train_epochs=5,per_device_train_batch_size=64,seed=8.1540\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.6482320797171326\n",
      "  pid: 579\n",
      "  time_since_restore: 28.820181846618652\n",
      "  time_this_iter_s: 28.820181846618652\n",
      "  time_total_s: 28.820181846618652\n",
      "  timestamp: 1676963624\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00000'\n",
      "  warmup_time: 0.0029206275939941406\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m {'train_runtime': 14.9842, 'train_samples_per_second': 489.582, 'train_steps_per_second': 20.422, 'train_loss': 0.6444006028518178, 'epoch': 2.0}\n",
      "Result for _objective_34426_00003:\n",
      "  date: 2023-02-20_23-13-58\n",
      "  done: true\n",
      "  epoch: 2.0\n",
      "  eval_accuracy: 0.6838235294117647\n",
      "  eval_f1: 0.8122270742358079\n",
      "  eval_loss: 0.6198461055755615\n",
      "  eval_runtime: 0.3198\n",
      "  eval_samples_per_second: 1275.67\n",
      "  eval_steps_per_second: 159.459\n",
      "  experiment_id: 9140650e7cff45c8adb6c69d83905350\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.4960506036475727\n",
      "  pid: 696\n",
      "  time_since_restore: 18.726917505264282\n",
      "  time_this_iter_s: 18.726917505264282\n",
      "  time_total_s: 18.726917505264282\n",
      "  timestamp: 1676963638\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00003'\n",
      "  warmup_time: 0.0025038719177246094\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=696)\u001b[0m {'eval_loss': 0.6198461055755615, 'eval_accuracy': 0.6838235294117647, 'eval_f1': 0.8122270742358079, 'eval_runtime': 0.3198, 'eval_samples_per_second': 1275.67, 'eval_steps_per_second': 159.459, 'epoch': 2.0}\n",
      "Result for _objective_34426_00002:\n",
      "  date: 2023-02-20_23-13-46\n",
      "  done: true\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.8284313725490197\n",
      "  eval_f1: 0.8817567567567567\n",
      "  eval_loss: 0.43873196840286255\n",
      "  eval_runtime: 0.3098\n",
      "  eval_samples_per_second: 1316.919\n",
      "  eval_steps_per_second: 164.615\n",
      "  experiment_id: 050b42c640224f4d94fbfdb514687454\n",
      "  experiment_tag: 2_learning_rate=0.0000,num_train_epochs=5,per_device_train_batch_size=16,seed=24.4435\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.7101881293057764\n",
      "  pid: 611\n",
      "  time_since_restore: 28.391945600509644\n",
      "  time_this_iter_s: 28.391945600509644\n",
      "  time_total_s: 28.391945600509644\n",
      "  timestamp: 1676963626\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00002'\n",
      "  warmup_time: 0.003077268600463867\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=611)\u001b[0m {'train_runtime': 38.3841, 'train_samples_per_second': 477.802, 'train_steps_per_second': 19.93, 'train_loss': 0.4327417710248162, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:06 (running for 00:00:52.85)\n",
      "Memory usage on this node: 14.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (3 PENDING, 3 RUNNING, 4 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | RUNNING    | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:11 (running for 00:00:57.86)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (3 PENDING, 3 RUNNING, 4 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | RUNNING    | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:16 (running for 00:01:02.87)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (3 PENDING, 3 RUNNING, 4 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | RUNNING    | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:21 (running for 00:01:07.88)\n",
      "Memory usage on this node: 15.8/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (3 PENDING, 3 RUNNING, 4 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00006 | RUNNING    | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |             |\n",
      "| _objective_34426_00007 | PENDING    |                  |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m {'train_runtime': 15.0335, 'train_samples_per_second': 487.975, 'train_steps_per_second': 20.354, 'train_loss': 0.5015098721373314, 'epoch': 2.0}\n",
      "Result for _objective_34426_00006:\n",
      "  date: 2023-02-20_23-14-22\n",
      "  done: true\n",
      "  epoch: 2.0\n",
      "  eval_accuracy: 0.8014705882352942\n",
      "  eval_f1: 0.8610634648370498\n",
      "  eval_loss: 0.42823073267936707\n",
      "  eval_runtime: 0.3092\n",
      "  eval_samples_per_second: 1319.46\n",
      "  eval_steps_per_second: 164.932\n",
      "  experiment_id: 8b02875ca1784d33862dd5fb87bb7121\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.6625340530723438\n",
      "  pid: 812\n",
      "  time_since_restore: 18.840264797210693\n",
      "  time_this_iter_s: 18.840264797210693\n",
      "  time_total_s: 18.840264797210693\n",
      "  timestamp: 1676963662\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00006'\n",
      "  warmup_time: 0.00350189208984375\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=812)\u001b[0m {'eval_loss': 0.42823073267936707, 'eval_accuracy': 0.8014705882352942, 'eval_f1': 0.8610634648370498, 'eval_runtime': 0.3092, 'eval_samples_per_second': 1319.46, 'eval_steps_per_second': 164.932, 'epoch': 2.0}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:28 (running for 00:01:14.85)\n",
      "Memory usage on this node: 15.1/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (2 PENDING, 3 RUNNING, 5 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |             |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |             |\n",
      "| _objective_34426_00007 | RUNNING    | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m {'loss': 0.4603, 'learning_rate': 2.051065189098671e-06, 'epoch': 3.27}\n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m {'loss': 0.6065, 'learning_rate': 8.002660134366055e-07, 'epoch': 3.27}\n",
      "Result for _objective_34426_00005:\n",
      "  date: 2023-02-20_23-14-29\n",
      "  done: false\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.8382352941176471\n",
      "  eval_f1: 0.8885135135135136\n",
      "  eval_loss: 0.4339991509914398\n",
      "  eval_runtime: 0.3133\n",
      "  eval_samples_per_second: 1302.391\n",
      "  eval_steps_per_second: 162.799\n",
      "  experiment_id: fea05adf046b4a3cafd55bd26a2bd470\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.7267488076311608\n",
      "  pid: 753\n",
      "  time_since_restore: 28.368152856826782\n",
      "  time_this_iter_s: 28.368152856826782\n",
      "  time_total_s: 28.368152856826782\n",
      "  timestamp: 1676963669\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00005'\n",
      "  warmup_time: 0.003108501434326172\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m {'eval_loss': 0.4339991509914398, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8885135135135136, 'eval_runtime': 0.3133, 'eval_samples_per_second': 1302.391, 'eval_steps_per_second': 162.799, 'epoch': 3.27}\n",
      "Result for _objective_34426_00004:\n",
      "  date: 2023-02-20_23-14-29\n",
      "  done: false\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.7083333333333334\n",
      "  eval_f1: 0.8226527570789866\n",
      "  eval_loss: 0.57361900806427\n",
      "  eval_runtime: 0.3169\n",
      "  eval_samples_per_second: 1287.482\n",
      "  eval_steps_per_second: 160.935\n",
      "  experiment_id: b8cbdb2620794c54bb23b8e29ba11e05\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.5309860904123198\n",
      "  pid: 752\n",
      "  time_since_restore: 28.607365369796753\n",
      "  time_this_iter_s: 28.607365369796753\n",
      "  time_total_s: 28.607365369796753\n",
      "  timestamp: 1676963669\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00004'\n",
      "  warmup_time: 0.001955747604370117\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m {'eval_loss': 0.57361900806427, 'eval_accuracy': 0.7083333333333334, 'eval_f1': 0.8226527570789866, 'eval_runtime': 0.3169, 'eval_samples_per_second': 1287.482, 'eval_steps_per_second': 160.935, 'epoch': 3.27}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:34 (running for 00:01:21.68)\n",
      "Memory usage on this node: 15.7/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (2 PENDING, 3 RUNNING, 5 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00005 | RUNNING    | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00007 | RUNNING    | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | PENDING    |                  |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "Result for _objective_34426_00005:\n",
      "  date: 2023-02-20_23-14-29\n",
      "  done: true\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.8382352941176471\n",
      "  eval_f1: 0.8885135135135136\n",
      "  eval_loss: 0.4339991509914398\n",
      "  eval_runtime: 0.3133\n",
      "  eval_samples_per_second: 1302.391\n",
      "  eval_steps_per_second: 162.799\n",
      "  experiment_id: fea05adf046b4a3cafd55bd26a2bd470\n",
      "  experiment_tag: 5_learning_rate=0.0000,num_train_epochs=4,per_device_train_batch_size=16,seed=1.8994\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.7267488076311608\n",
      "  pid: 753\n",
      "  time_since_restore: 28.368152856826782\n",
      "  time_this_iter_s: 28.368152856826782\n",
      "  time_total_s: 28.368152856826782\n",
      "  timestamp: 1676963669\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00005'\n",
      "  warmup_time: 0.003108501434326172\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=753)\u001b[0m {'train_runtime': 31.2002, 'train_samples_per_second': 470.253, 'train_steps_per_second': 19.615, 'train_loss': 0.4354888005973467, 'epoch': 4.0}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:41 (running for 00:01:27.85)\n",
      "Memory usage on this node: 14.9/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (1 PENDING, 3 RUNNING, 6 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00004 | RUNNING    | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00007 | RUNNING    | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | RUNNING    | 192.168.48.4:917 |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | PENDING    |                  |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00005 | TERMINATED | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for _objective_34426_00004:\n",
      "  date: 2023-02-20_23-14-29\n",
      "  done: true\n",
      "  epoch: 3.27\n",
      "  eval_accuracy: 0.7083333333333334\n",
      "  eval_f1: 0.8226527570789866\n",
      "  eval_loss: 0.57361900806427\n",
      "  eval_runtime: 0.3169\n",
      "  eval_samples_per_second: 1287.482\n",
      "  eval_steps_per_second: 160.935\n",
      "  experiment_id: b8cbdb2620794c54bb23b8e29ba11e05\n",
      "  experiment_tag: 4_learning_rate=0.0000,num_train_epochs=5,per_device_train_batch_size=8,seed=25.0818\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.5309860904123198\n",
      "  pid: 752\n",
      "  time_since_restore: 28.607365369796753\n",
      "  time_this_iter_s: 28.607365369796753\n",
      "  time_total_s: 28.607365369796753\n",
      "  timestamp: 1676963669\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00004'\n",
      "  warmup_time: 0.001955747604370117\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=752)\u001b[0m {'train_runtime': 39.1066, 'train_samples_per_second': 468.974, 'train_steps_per_second': 19.562, 'train_loss': 0.5937084322661356, 'epoch': 5.0}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:49 (running for 00:01:35.87)\n",
      "Memory usage on this node: 15.1/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3.0/8 CPUs, 3.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (3 RUNNING, 7 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00007 | RUNNING    | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |             |\n",
      "| _objective_34426_00008 | RUNNING    | 192.168.48.4:917 |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | RUNNING    | 192.168.48.4:956 |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00004 | TERMINATED | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00005 | TERMINATED | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m /home/ray/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m {'train_runtime': 22.4566, 'train_samples_per_second': 490.011, 'train_steps_per_second': 20.439, 'train_loss': 0.5574849114179091, 'epoch': 3.0}\n",
      "Result for _objective_34426_00007:\n",
      "  date: 2023-02-20_23-14-51\n",
      "  done: true\n",
      "  epoch: 3.0\n",
      "  eval_accuracy: 0.7132352941176471\n",
      "  eval_f1: 0.821917808219178\n",
      "  eval_loss: 0.5029550194740295\n",
      "  eval_runtime: 0.3661\n",
      "  eval_samples_per_second: 1114.549\n",
      "  eval_steps_per_second: 139.319\n",
      "  experiment_id: f2637d22e92447e8b0600cf2edc7fb12\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.535153102336825\n",
      "  pid: 869\n",
      "  time_since_restore: 26.227601528167725\n",
      "  time_this_iter_s: 26.227601528167725\n",
      "  time_total_s: 26.227601528167725\n",
      "  timestamp: 1676963691\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00007'\n",
      "  warmup_time: 0.0021789073944091797\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=869)\u001b[0m {'eval_loss': 0.5029550194740295, 'eval_accuracy': 0.7132352941176471, 'eval_f1': 0.821917808219178, 'eval_runtime': 0.3661, 'eval_samples_per_second': 1114.549, 'eval_steps_per_second': 139.319, 'epoch': 3.0}\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:14:56 (running for 00:01:43.24)\n",
      "Memory usage on this node: 13.4/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/8 CPUs, 2.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00008 | RUNNING    | 192.168.48.4:917 |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | RUNNING    | 192.168.48.4:956 |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00004 | TERMINATED | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00005 | TERMINATED | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "| _objective_34426_00007 | TERMINATED | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |     1.53515 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:15:01 (running for 00:01:48.25)\n",
      "Memory usage on this node: 13.4/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/8 CPUs, 2.0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00008 | RUNNING    | 192.168.48.4:917 |     1.53049e-05 |                  3 |                     64 | 34.5377  |             |\n",
      "| _objective_34426_00009 | RUNNING    | 192.168.48.4:956 |     7.96157e-06 |                  2 |                     32 | 38.0065  |             |\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00004 | TERMINATED | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00005 | TERMINATED | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "| _objective_34426_00007 | TERMINATED | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |     1.53515 |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m {'train_runtime': 22.7648, 'train_samples_per_second': 483.377, 'train_steps_per_second': 20.163, 'train_loss': 0.4324929231132557, 'epoch': 3.0}\n",
      "Result for _objective_34426_00008:\n",
      "  date: 2023-02-20_23-15-04\n",
      "  done: true\n",
      "  epoch: 3.0\n",
      "  eval_accuracy: 0.8333333333333334\n",
      "  eval_f1: 0.8823529411764706\n",
      "  eval_loss: 0.3925471603870392\n",
      "  eval_runtime: 0.3133\n",
      "  eval_samples_per_second: 1302.474\n",
      "  eval_steps_per_second: 162.809\n",
      "  experiment_id: 790c367e107046cd855bee4af0a56920\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.715686274509804\n",
      "  pid: 917\n",
      "  time_since_restore: 26.516175031661987\n",
      "  time_this_iter_s: 26.516175031661987\n",
      "  time_total_s: 26.516175031661987\n",
      "  timestamp: 1676963704\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00008'\n",
      "  warmup_time: 0.002447843551635742\n",
      "  \n",
      "\u001b[2m\u001b[36m(_objective pid=917)\u001b[0m {'eval_loss': 0.3925471603870392, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8823529411764706, 'eval_runtime': 0.3133, 'eval_samples_per_second': 1302.474, 'eval_steps_per_second': 162.809, 'epoch': 3.0}\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m {'train_runtime': 15.5355, 'train_samples_per_second': 472.21, 'train_steps_per_second': 19.697, 'train_loss': 0.5654170715730954, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 23:15:05,645\tINFO tune.py:758 -- Total run time: 112.52 seconds (112.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for _objective_34426_00009:\n",
      "  date: 2023-02-20_23-15-05\n",
      "  done: true\n",
      "  epoch: 2.0\n",
      "  eval_accuracy: 0.7009803921568627\n",
      "  eval_f1: 0.8140243902439025\n",
      "  eval_loss: 0.5210139751434326\n",
      "  eval_runtime: 0.3137\n",
      "  eval_samples_per_second: 1300.652\n",
      "  eval_steps_per_second: 162.582\n",
      "  experiment_id: f26a15f2acc740f6a26eea7ac9c8742d\n",
      "  hostname: 3b3c85b4f075\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.48.4\n",
      "  objective: 1.5150047824007653\n",
      "  pid: 956\n",
      "  time_since_restore: 19.2745418548584\n",
      "  time_this_iter_s: 19.2745418548584\n",
      "  time_total_s: 19.2745418548584\n",
      "  timestamp: 1676963705\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '34426_00009'\n",
      "  warmup_time: 0.0025255680084228516\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2023-02-20 23:15:05 (running for 00:01:52.30)\n",
      "Memory usage on this node: 11.0/251.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/3 GPUs, 0.0/46.31 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/jovyan/ray_results/_objective_2023-02-20_23-13-13\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "| Trial name             | status     | loc              |   learning_rate |   num_train_epochs |   per_device_train_... |     seed |   objective |\n",
      "|------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------|\n",
      "| _objective_34426_00000 | TERMINATED | 192.168.48.4:579 |     5.61152e-06 |                  5 |                     64 |  8.15396 |     1.64823 |\n",
      "| _objective_34426_00001 | TERMINATED | 192.168.48.4:609 |     1.56207e-05 |                  2 |                     16 |  7.08379 |     1.69363 |\n",
      "| _objective_34426_00002 | TERMINATED | 192.168.48.4:611 |     8.28892e-06 |                  5 |                     16 | 24.4435  |     1.71019 |\n",
      "| _objective_34426_00003 | TERMINATED | 192.168.48.4:696 |     1.09943e-06 |                  2 |                      8 | 29.158   |     1.49605 |\n",
      "| _objective_34426_00004 | TERMINATED | 192.168.48.4:752 |     2.3102e-06  |                  5 |                      8 | 25.0818  |     1.53099 |\n",
      "| _objective_34426_00005 | TERMINATED | 192.168.48.4:753 |     1.12076e-05 |                  4 |                     16 |  1.89943 |     1.72675 |\n",
      "| _objective_34426_00006 | TERMINATED | 192.168.48.4:812 |     1.67381e-05 |                  2 |                     32 |  2.81996 |     1.66253 |\n",
      "| _objective_34426_00007 | TERMINATED | 192.168.48.4:869 |     5.4041e-06  |                  3 |                     32 | 15.916   |     1.53515 |\n",
      "| _objective_34426_00008 | TERMINATED | 192.168.48.4:917 |     1.53049e-05 |                  3 |                     64 | 34.5377  |     1.71569 |\n",
      "| _objective_34426_00009 | TERMINATED | 192.168.48.4:956 |     7.96157e-06 |                  2 |                     32 | 38.0065  |     1.515   |\n",
      "+------------------------+------------+------------------+-----------------+--------------------+------------------------+----------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(_objective pid=956)\u001b[0m {'eval_loss': 0.5210139751434326, 'eval_accuracy': 0.7009803921568627, 'eval_f1': 0.8140243902439025, 'eval_runtime': 0.3137, 'eval_samples_per_second': 1300.652, 'eval_steps_per_second': 162.582, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='34426_00005', objective=1.7267488076311608, hyperparameters={'learning_rate': 1.1207606211860595e-05, 'num_train_epochs': 4, 'seed': 1.8994345766152145, 'per_device_train_batch_size': 16})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "dataset = load_dataset('glue', 'mrpc')\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def encode(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples['sentence1'], examples['sentence2'], truncation=True)\n",
    "    return outputs\n",
    "\n",
    "encoded_dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Evaluate during training and a bit more often\n",
    "# than the default to be able to prune bad trials early.\n",
    "# Disabling tqdm is a matter of preference.\n",
    "training_args = TrainingArguments(\n",
    "    \"test\", evaluation_strategy=\"steps\", eval_steps=500, disable_tqdm=True)\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Default objective is the sum of all metrics\n",
    "# when metrics are provided, so we have to maximize it.\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"ray\", \n",
    "    n_trials=10 # number of trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5522dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
